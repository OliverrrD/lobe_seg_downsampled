{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/nfs/masi/gaor2/saved_file/CNNLung/rgnet/3split_margin/result/test_combineall_lossmin_sort.csv')\n",
    "df0 = pd.read_csv('/nfs/masi/gaor2/saved_file/CNNLung/rgnet/3split_base/result/test_combineall_lossmin_sort.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1529, 49], [82, 627]]\n"
     ]
    }
   ],
   "source": [
    "assert df0['gt_list'].tolist() == df1['gt_list'].tolist()\n",
    "\n",
    "pred0_list = df0['prob_list'].tolist()\n",
    "pred1_list = df1['prob_list'].tolist()\n",
    "\n",
    "pred0_list = [int(i > 0.5) for i in pred0_list] #0.19678\n",
    "pred1_list = [int(i > 0.5) for i in pred1_list] #0.25428\n",
    "\n",
    "\n",
    "t0_0 = sum([int(pred0_list[i] == gt_list[i] and pred1_list[i] == gt_list[i]) for i in range(len(gt_list))])\n",
    "\n",
    "t1_0 = sum([int(pred0_list[i] != gt_list[i] and pred1_list[i] == gt_list[i]) for i in range(len(gt_list))])\n",
    "\n",
    "t0_1 = sum([int(pred0_list[i] == gt_list[i] and pred1_list[i] != gt_list[i]) for i in range(len(gt_list))])\n",
    "\n",
    "t1_1 = sum([int(pred0_list[i] != gt_list[i] and pred1_list[i] != gt_list[i]) for i in range(len(gt_list))])\n",
    "\n",
    "table = [[t0_0, t0_1], [t1_0, t1_1]]\n",
    "print (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.689986882378662"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([int(pred0_list[i] == gt_list[i]) for i in range(len(gt_list))]) / len(gt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3058, 164], [98, 1254]]\n",
      "statistic=12.000, p-value=0.065\n"
     ]
    }
   ],
   "source": [
    "table =  [[1529 * 2, 82 * 2], [49 * 2, 627 * 2]]\n",
    "print (table)\n",
    "table = [[152 *3 , 8 * 3], [4 * 3, 62 * 3]]\n",
    "result = mcnemar(table, exact=True)\n",
    "\n",
    "print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap\n",
    "#https://github.com/mateuszbuda/ml-stat-util/blob/master/stat_util.py\n",
    "import numpy as np\n",
    "from scipy.stats import percentileofscore\n",
    "def pvalue(\n",
    "    y_true,\n",
    "    y_pred1,\n",
    "    y_pred2,\n",
    "    score_fun,\n",
    "    n_bootstraps=2000,\n",
    "    two_tailed=True,\n",
    "    seed=None,\n",
    "    reject_one_class_samples=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute p-value for hypothesis that score function for model I predictions is higher than for model II predictions\n",
    "    using bootstrapping.\n",
    "    :param y_true: 1D list or array of labels.\n",
    "    :param y_pred1: 1D list or array of predictions for model I corresponding to elements in y_true.\n",
    "    :param y_pred2: 1D list or array of predictions for model II corresponding to elements in y_true.\n",
    "    :param score_fun: Score function for which confidence interval is computed. (e.g. sklearn.metrics.accuracy_score)\n",
    "    :param n_bootstraps: The number of bootstraps. (default: 2000)\n",
    "    :param two_tailed: Whether to use two-tailed test. (default: True)\n",
    "    :param seed: Random seed for reproducibility. (default: None)\n",
    "    :param reject_one_class_samples: Whether to reject bootstrapped samples with only one label. For scores like AUC we\n",
    "    need at least one positive and one negative sample. (default: True)\n",
    "    :return: Computed p-value, array of bootstrapped differences of scores.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(y_true) == len(y_pred1)\n",
    "    assert len(y_true) == len(y_pred2)\n",
    "\n",
    "    return pvalue_stat(\n",
    "        y_true=y_true,\n",
    "        y_preds1=y_pred1,\n",
    "        y_preds2=y_pred2,\n",
    "        score_fun=score_fun,\n",
    "        n_bootstraps=n_bootstraps,\n",
    "        two_tailed=two_tailed,\n",
    "        seed=seed,\n",
    "        reject_one_class_samples=reject_one_class_samples,\n",
    "    )\n",
    "\n",
    "\n",
    "def pvalue_stat(\n",
    "    y_true,\n",
    "    y_preds1,\n",
    "    y_preds2,\n",
    "    score_fun,\n",
    "    stat_fun=np.mean,\n",
    "    n_bootstraps=2000,\n",
    "    two_tailed=True,\n",
    "    seed=None,\n",
    "    reject_one_class_samples=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute p-value for hypothesis that given statistic of score function for model I predictions is higher than for\n",
    "    model II predictions using bootstrapping.\n",
    "    :param y_true: 1D list or array of labels.\n",
    "    :param y_preds1: A list of lists or 2D array of predictions for model I corresponding to elements in y_true.\n",
    "    :param y_preds2: A list of lists or 2D array of predictions for model II corresponding to elements in y_true.\n",
    "    :param score_fun: Score function for which confidence interval is computed. (e.g. sklearn.metrics.accuracy_score)\n",
    "    :param stat_fun: Statistic for which p-value is computed. (e.g. np.mean)\n",
    "    :param n_bootstraps: The number of bootstraps. (default: 2000)\n",
    "    :param two_tailed: Whether to use two-tailed test. (default: True)\n",
    "    :param seed: Random seed for reproducibility. (default: None)\n",
    "    :param reject_one_class_samples: Whether to reject bootstrapped samples with only one label. For scores like AUC we\n",
    "    need at least one positive and one negative sample. (default: True)\n",
    "    :return: Computed p-value, array of bootstrapped differences of scores.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_preds1 = np.atleast_2d(y_preds1)\n",
    "    y_preds2 = np.atleast_2d(y_preds2)\n",
    "    assert all(len(y_true) == len(y) for y in y_preds1)\n",
    "    assert all(len(y_true) == len(y) for y in y_preds2)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    z = []\n",
    "    for i in range(n_bootstraps):\n",
    "        readers1 = np.random.randint(0, len(y_preds1), len(y_preds1))\n",
    "        readers2 = np.random.randint(0, len(y_preds2), len(y_preds2))\n",
    "        indices = np.random.randint(0, len(y_true), len(y_true))\n",
    "        if reject_one_class_samples and len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        reader_scores = []\n",
    "        for r in readers1:\n",
    "            reader_scores.append(score_fun(y_true[indices], y_preds1[r][indices]))\n",
    "        score1 = stat_fun(reader_scores)\n",
    "        reader_scores = []\n",
    "        for r in readers2:\n",
    "            reader_scores.append(score_fun(y_true[indices], y_preds2[r][indices]))\n",
    "        score2 = stat_fun(reader_scores)\n",
    "        z.append(score1 - score2)\n",
    "\n",
    "    p = percentileofscore(z, 0.0, kind=\"weak\") / 100.0\n",
    "    if two_tailed:\n",
    "        p *= 2.0\n",
    "    return p, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989 0.011\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/nfs/masi/gaor2/saved_file/Clinical_spore/0611/external/combine_ext.csv')\n",
    "gt_list = df['target'].tolist() \n",
    "pred0_list = df['kaggle'].tolist() \n",
    "pred1_list = df['pred'].tolist() \n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "p, z = pvalue(gt_list, pred1_list, pred0_list, score_fun=roc_auc_score)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(len(z)):\n",
    "    if z[i] > 0:\n",
    "        cnt += 1\n",
    "print (cnt, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1958 0.042\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('/nfs/masi/gaor2/saved_file/Clinical_spore/multipath/0712/external/average.csv')\n",
    "df0 = pd.read_csv('/nfs/masi/gaor2/saved_file/Clinical_spore/multipath/0712/onlyboth/train4fold_SGD/average.csv')\n",
    "\n",
    "#df1 = df1.query('diag < 0 or target == 0')\n",
    "#df0 = df0.query('diag < 0 or target == 0')\n",
    "\n",
    "assert df0['BothTar'].tolist() == df1['BothTar'].tolist()\n",
    "gt_list = df0['BothTar'].tolist() #* 5\n",
    "\n",
    "pred0_list = df0['BOTHPred'].tolist()#* 5\n",
    "pred1_list = df1['BOTHPred'].tolist() #* 5\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "p, z = pvalue(gt_list, pred1_list, pred0_list, score_fun=roc_auc_score)\n",
    "cnt = 0\n",
    "for i in range(len(z)):\n",
    "    if z[i] > 0:\n",
    "        cnt += 1\n",
    "print (cnt, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 220\n",
      "1988 0.012\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('/nfs/masi/gaor2/saved_file/Clinical_spore/0622/all/mean.csv')\n",
    "ID1 = df1['ID_list'].tolist()\n",
    "ID1 = [i.replace('.npy', '') for i in ID1]\n",
    "\n",
    "df = pd.read_csv('/nfs/masi/gaor2/data/Cotrain/clinical_spore/spore/norm/spore_with2yr.csv')\n",
    "df = df.query('plco == plco')\n",
    "df = df.loc[df['id'] != '00000981time20180709']\n",
    "kaggle_pred = df['cancer'].tolist()\n",
    "tar = df['gt_reg'].tolist() \n",
    "ID = df['id'].tolist()\n",
    "\n",
    "assert ID1 == ID\n",
    "\n",
    "pred1_list = df1['pred'].tolist() #* 5\n",
    "tar1_list = df1['target'].tolist() #* 5\n",
    "print (len(tar), len(tar1_list))\n",
    "assert tar == tar1_list\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "p, z = pvalue(tar1_list * 5, pred1_list * 5, kaggle_pred * 5, score_fun=roc_auc_score)\n",
    "cnt = 0\n",
    "for i in range(len(z)):\n",
    "    if z[i] > 0:\n",
    "        cnt += 1\n",
    "print (cnt, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      00000009time20160420.npy\n",
       "1      00000010time20140214.npy\n",
       "2      00000010time20160127.npy\n",
       "3      00000015time20140307.npy\n",
       "4      00000015time20150713.npy\n",
       "                 ...           \n",
       "215    00001082time20180131.npy\n",
       "216    00001104time20130130.npy\n",
       "217    00001104time20140312.npy\n",
       "218    00001104time20150820.npy\n",
       "219    00001112time20130322.npy\n",
       "Name: ID_list, Length: 220, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['ID_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python37]",
   "language": "python",
   "name": "conda-env-python37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
